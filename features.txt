group query attention
swiglu
rotary positional embeddings
rmsnorm
yarn scaling during train time
pre-normalization
qk norm
abf
dual chunk attention
training in 8-bit
mla-kv cache compression and decompression